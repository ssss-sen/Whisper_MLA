# TrainingArguments
seed: 42
max_steps: 18000
report_to: none
run_name: 2norm-train
save_strategy : steps
save_steps: 18000
output_dir: "output/uniform_model/"
overwrite_output_dir: true
logging_strategy: steps
logging_steps: 100
resume_from_checkpoint: null
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
remove_unused_columns: False
gradient_accumulation_steps: 4
bf16: true
deepspeed: "configs/ds_config.json"
dataloader_drop_last: true
# optim
optim: adamw_torch
max_grad_norm: 1.0
learning_rate: 1.0e-4
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-8
weight_decay: 0.01
# lr scheduler
use_constant_with_warmup_decay_scheduler: true
lr_scheduler_kwargs: {"lr_decay_starting_step": 16200, "lr_decay_steps": 1800, "lr_decay_style": "1-sqrt", "lr_warmup_steps": 900, "lr_warmup_style": "linear", "min_decay_lr": 0}

# ModelArguments
model_name_or_path: path_to_whisper_model
partial_rope_version: 'uniform' # 'high'/'low'/'uniform'/'2-norm'
rope_dim_for_mla: 48
uniform_start_point: 0 # optional only for 'uniform'
# num_query_heads_div_key_heads: 1
#qk_tensor_path: path_to_qk_tensor
svd_init_method: joint # 'split'/'joint'
low_rank: 8
is_baseline: false
is_gqa2mha2mla: false
tokenizer_name_or_path: path_to_whisper_model

# DataArguments
train_ann_path: "data/train.json"
whisper_path: path_to_whisper_model
sequence_length: 2048